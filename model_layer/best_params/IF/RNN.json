{
    "epochs": 10,
    "batch_size": 64,
    "hidden_size": 128,
    "seq_length": 32,
    "weight_decay": 0.03699014272607559,
    "num_layers": 2,
    "fc_size": 128,
    "learning_rate": 0.006264079267383521,
    "dropout_prob": 0.0049846528896436
}